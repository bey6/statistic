# 关于 es 集群节点文档数量与索引文档数量不一致问题的分析

## 文档信息

- 文档创建人：刘备
- 文档创建日期：2021-01-27 10:22:29

## 现象

在 2021-01-26 日，对于统计系统的性能分析的过程中，偶然间发现 kibana 中对于 mrfs 索引的总量统计与集群节点调用 es api 获得的文档数量不一致；因此，为了进一步确定问题是否存在，我在统计系统中检索了姓名 > 0 岁的患者，结果与 es api 统计的文档总量一致，与 kibana 中统计的索引文档数不一致。

虽然证据稍显不足，但基本上可以断言统计系统的 ES 集群搭建存在问题。

**ES 集群文档数量仅仅包含 20 万！**

## 思路

该如何解决问题？

**1. 尝试重启集群节点**

无效。

**2. 尝试从分片的基础知识开始分析**

ES 的 Index 是由 Shards 组成的，然而每个 Shard 由都是一个独立的 Lucene 引擎，所以每个 Search Request 在进行请求时，每一个 Shard 都会进行独立的检索。同时 Shard 有如下特征：

- 默认情况下，每个 Index 会自动创建 10 个分片，分别是 5 个主分片、5 个副本分片
- 主分片与副本分片有不可被分配到同一个节点的规则限制，所以仅有一个节点时，集群健康状况一定时 Yellow
- ES 会尽可能的平均分配 Primary Shard，假如你有 3 台数据节点，那么会按照 221 的方式分配 Primary Shard，这就是所谓的 Rebalance
- Primary Shard 会根据节点的变化而动态的重新分配，这个机制叫做 Relocation

_所以，分片基本上是自动均衡的，在集群节点的加入与退出时，又有 Relocation + Rebalance 机制进行重新均衡，因此问题可能不在这里。_

**3. 尝试从集群的基础知识开始分析**

一群 es 节点组织成一个集群，每一个 es 节点代表一个 es 实例（一个机器可以有多个 es 实例）。当同一个网络中存在集群名称相同的 es 节点时，他们就会形成一个集群。不过集群中的节点也会有不同的角色划分，包括：

- master node(node.master=true): es 设计了相应的选主机制来推举 master，master 理论上*不应该*是一个 data node，master 应该只负责调度、管理节点层面的问题，而不会落实的检索层面的工作，`kibana` 一般会部署在 master 节点上
- data node(node.data=true): 干活的，跑检索任务
- client node(node.master=false;node.data=false): 负责请求的协调、分发、汇总，不具备检索能力，一般需要处理高并发的请求时才会启用
- tribe node: 可简单理解为跨集群的 master，7.0 后移除
- coordinating node: 不是一个实际的节点，每个节点都可以是协调节点，也就是每个节点收到请求后都会将请求协调（分发）给其他的节点

_无法断定问题是否出在这里，但是明白了 es 的 master 节点可以把 node.data 设置为 false。_

**4. 为什么 Head 插件中会看到两个文档数量？**

```
mrfs
size: 1.69Gi (3.38Gi)
docs: 2,068,316 (4,140,530)
```

| 方式                                        | 索引 | 数量    |
| ------------------------------------------- | ---- | ------- |
| http://172.30.199.41:9200/_cat/indices/mrfs | mrfs | 2068316 |
| http://172.30.199.41:9200/mrfs/_count       | mrfs | 248840  |

为什么差了这么多？因为 ES 统计文档数量有两种方式，一种是按照 Lucene 统计，另一种是按照 ES 文档统计；

- Lucene: nested 类型会计算为单独的文档
- ES: 只按照跟文档的数量计算

经查阅 248840 是根级别计算的文档量，2068316 是连 nested 子文档一起计算的结果，所以数量会更多。

结论是文档数量是正确的，就是只有 20 万，那么为什么数据导入脚本显示的总量有 146 万只多，结果却仅仅有 20 万？

又是一轮检查，最后找到了原因，是因为 elasticsearch 包中的 bulk 函数在处理批量脚本时，会出现静默失败的问题，导致数据同步静默丢失了，这也就是为什么到最后显示的文档量不足，但是又没看到什么报错的原因。

有部分社区成员解释可以添加一些额外测参数给 bulk 可以解决问题，但是尝试后均无效，最终还是改为单条插入解决了问题。

## 参考

- https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster
- https://juejin.cn/post/6844904056347951117
